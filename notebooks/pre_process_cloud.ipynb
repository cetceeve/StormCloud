{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Cloud outage data into dataframe\n",
    "\n",
    "First import the json and define column names and there mapping to json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"../json-data/incidents.json\") as file:\n",
    "    incidents_json = json.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create data frame and copy over the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "COLS = {\"id\": \"id\",    \n",
    "        \"start\": \"begin\",\n",
    "        \"end\": \"end\",\n",
    "        \"severity\": \"severity\",\n",
    "        \"status\": \"status_impact\",\n",
    "        \"location\": (\"currently_affected_locations\", \"previously_affected_locations\"),\n",
    "        \"service_name\": \"service_name\",\n",
    "        \"number_affected_products\": \"affected_products\",\n",
    "    }\n",
    "\n",
    "df_incidents = pd.DataFrame(data=[], columns=COLS.keys())\n",
    "\n",
    "def add1hour(starttime: str):\n",
    "    converted = datetime.strptime(starttime, '%Y-%m-%dT%H:%M:%S+00:00')\n",
    "    #print(converted)\n",
    "    time_change = timedelta(minutes=60)\n",
    "    new_time = converted + time_change\n",
    "    #print(new_time)\n",
    "    endtime = new_time.strftime('%Y-%m-%dT%H:%M:%S+00:00')\n",
    "    #print(endtime)\n",
    "    return endtime\n",
    "\n",
    "for incident in incidents_json:\n",
    "    locations = [x[\"id\"] for x in incident.get(COLS[\"location\"][0]) + incident.get(COLS[\"location\"][1], None)]\n",
    "    # print([item for item in locations if item not in data_centers[\"id\"].to_list()])\n",
    "    # countries = [data_centers.at[id, \"country\"] for id in locations]\n",
    "    \n",
    "    row = {\n",
    "        \"id\": incident.get(COLS[\"id\"], None),\n",
    "        \"start\": incident.get(COLS[\"start\"], None),\n",
    "        \"end\": incident.get(COLS[\"end\"], add1hour(incident.get(COLS[\"start\"]))),\n",
    "        \"severity\": incident.get(COLS[\"severity\"], None),\n",
    "        \"location\": locations,\n",
    "        # \"country\": countries,\n",
    "        \"service_name\": incident.get(COLS[\"service_name\"]),\n",
    "        \"status\": incident.get(COLS[\"status\"], None),\n",
    "        \"number_affected_products\": len(incident.get(COLS[\"number_affected_products\"], []))\n",
    "    }\n",
    "    df_incidents.loc[len(df_incidents)] = row\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explode list of countries into multiple rows and add datacenter information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_centers = pd.read_csv(\"../data/data-centers-rough.csv\", dtype={\n",
    "    'city': 'string','country':'string','id':'string'})\n",
    "data_centers = data_centers.set_index(\"id\")\n",
    "data_centers.index.name = 'location'\n",
    "\n",
    "df_exploded = df_incidents.explode(\"location\")\n",
    "df_incidentsWithLocation = pd.merge(df_exploded, data_centers, how=\"left\", on=\"location\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop duplicate countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3868, 10)\n",
      "(2644, 10)\n"
     ]
    }
   ],
   "source": [
    "print(df_incidentsWithLocation.shape)\n",
    "df_incidentsWithCountry = df_incidentsWithLocation.drop_duplicates(subset=[\"id\", \"country\"])\n",
    "print(df_incidentsWithCountry.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create incidents table with incidents per day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fc/nmrtwsyj4cx9_51gmbxm2lb80000gn/T/ipykernel_83097/2275863993.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_incidentsWithCountry['from1'] = pd.to_datetime(df_incidentsWithCountry['start']).dt.date # dt.date looses hours and minutes information\n",
      "/var/folders/fc/nmrtwsyj4cx9_51gmbxm2lb80000gn/T/ipykernel_83097/2275863993.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_incidentsWithCountry['to1'] = pd.to_datetime(df_incidentsWithCountry['end']).dt.date # dt.date looses hours and minutes information\n"
     ]
    }
   ],
   "source": [
    "# Expand DataFrame from range of dates to one day per row\n",
    "df_incidentsWithCountry['from1'] = pd.to_datetime(df_incidentsWithCountry['start']).dt.date # dt.date looses hours and minutes information\n",
    "df_incidentsWithCountry['to1'] = pd.to_datetime(df_incidentsWithCountry['end']).dt.date # dt.date looses hours and minutes information\n",
    "L = [pd.Series(r.id, pd.date_range(r.from1, r.to1, freq='D')) for r in df_incidentsWithCountry.itertuples()]\n",
    "df_idOnDateRange = pd.concat(L).reset_index()\n",
    "df_idOnDateRange.columns = ['date', 'id']\n",
    "# duplicates happended because of exploded country rows\n",
    "df_idOnDateRange = df_idOnDateRange.drop_duplicates([\"date\", \"id\"])\n",
    "\n",
    "df_incidentsPerDay = pd.merge(df_idOnDateRange, df_incidentsWithCountry, on=\"id\", how=\"inner\")\n",
    "df_incidentsPerDay = df_incidentsPerDay.drop(columns=[\"from1\", \"to1\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(636, 2)\n",
      "(5723, 11)\n"
     ]
    }
   ],
   "source": [
    "print(df_idOnDateRange.shape)\n",
    "print(df_incidentsPerDay.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create dataframe with row for each day x country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10304, 2)\n",
      "(11726, 11)\n"
     ]
    }
   ],
   "source": [
    "# Create df with only days\n",
    "dates = pd.date_range(start='2022-09-15', end='2023-09-17', freq='D').to_frame(name='date')\n",
    "# create df with only countries\n",
    "countries = df_incidentsWithCountry[\"country\"].drop_duplicates()\n",
    "dates_countries = dates.merge(countries, how='cross')\n",
    "print(dates_countries.shape)\n",
    "\n",
    "df = pd.merge(dates_countries, df_incidentsPerDay, on=['date', 'country'], how='left')\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        date      country                    id                      start  \\\n",
      "0 2022-09-15       Taiwan  urNR4xD4gBNsyaZj3W1i  2022-09-15T23:01:57+00:00   \n",
      "1 2022-09-15    Hong Kong  urNR4xD4gBNsyaZj3W1i  2022-09-15T23:01:57+00:00   \n",
      "2 2022-09-15        Japan  urNR4xD4gBNsyaZj3W1i  2022-09-15T23:01:57+00:00   \n",
      "3 2022-09-15  South Korea  urNR4xD4gBNsyaZj3W1i  2022-09-15T23:01:57+00:00   \n",
      "4 2022-09-15        India  urNR4xD4gBNsyaZj3W1i  2022-09-15T23:01:57+00:00   \n",
      "\n",
      "                         end severity               status         location  \\\n",
      "0  2022-09-29T20:49:04+00:00      low  SERVICE_INFORMATION       asia-east1   \n",
      "1  2022-09-29T20:49:04+00:00      low  SERVICE_INFORMATION       asia-east2   \n",
      "2  2022-09-29T20:49:04+00:00      low  SERVICE_INFORMATION  asia-northeast1   \n",
      "3  2022-09-29T20:49:04+00:00      low  SERVICE_INFORMATION  asia-northeast3   \n",
      "4  2022-09-29T20:49:04+00:00      low  SERVICE_INFORMATION      asia-south1   \n",
      "\n",
      "               service_name  number_affected_products             city  \n",
      "0  Google Kubernetes Engine                       1.0  Changhua County  \n",
      "1  Google Kubernetes Engine                       1.0        Hong Kong  \n",
      "2  Google Kubernetes Engine                       1.0            Tokyo  \n",
      "3  Google Kubernetes Engine                       1.0            Seoul  \n",
      "4  Google Kubernetes Engine                       1.0           Mumbai  \n"
     ]
    }
   ],
   "source": [
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Export as csv table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"../data/incidentsByDateCountry.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
